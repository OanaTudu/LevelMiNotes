{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib.request import urlopen, urlretrieve, quote, URLopener\n",
    "import requests\n",
    "import lxml\n",
    "import os\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinks(url):\n",
    "    #get the content\n",
    "    html_page =requests.get(url).content\n",
    "    soup = BeautifulSoup(html_page)\n",
    "    #initialize list of urls to files\n",
    "    links = []\n",
    "    #loop through the sectionLinks snippet to extract the urls   soup.find_all('div', id=\"sectionLinks\")\n",
    "    links = [link.get('href') for link in soup.findAll('a') if link and link.parent.name == 'div']\n",
    "    return links\n",
    "#let's try it on a page from Hausmusik \n",
    "#print( getLinks(\"http://www.hausmusik.ch/notenregal/b/\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.hausmusik.ch/notenregal/\"\n",
    "first_page_urls = getLinks(url)\n",
    "#initialize the list of urls:\n",
    "file_urls = []\n",
    "#go through each url:\n",
    "for purl in first_page_urls:\n",
    "    if purl.count('/')  == 1:\n",
    "        n_url = url + s_url\n",
    "        #retrieve the href links from the page\n",
    "        spu = getLinks(n_url)\n",
    "        #construct the full path for each link retrieved:\n",
    "        spu_link = [n_url+local_link for local_link in spu if local_link.count('/') == 1]\n",
    "        second_page_urls+=spu_link\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's go one level deeper and start checking for files:\n",
    "third_page_urls =[]\n",
    "for t_url in second_page_urls:\n",
    "        #retrieve the href links from the page\n",
    "        tpu = getLinks(t_url)\n",
    "        #contruct the full path for each link retrieved:\n",
    "        tpu_link = [t_url+local_link for local_link in tpu if local_link.count('/') == 1]\n",
    "        if tpu_link: \n",
    "            third_page_urls+=tpu_link\n",
    "        \n",
    "        elif '.xml' in t_url:  #reached the file I need -> download it\n",
    "            file_name = 'musicXMLFiles/' + os.path.basename(t_url)\n",
    "            try:\n",
    "                urlretrieve(t_url, file_name)\n",
    "                print(f'downloaded file {file_name}')\n",
    "            except:\n",
    "                print('failed to download')   \n",
    "            time.sleep(1)\n",
    "        elif '.' in t_url[-4:-1]:\n",
    "            print('no xml file, just other formats') \n",
    "#print(third_page_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's go one level deeper and start checking for files:\n",
    "fourth_page_urls =[]\n",
    "for f_url in third_page_urls:\n",
    "    print(f_url)\n",
    "    #retrieve the href links from the page\n",
    "    fpu = getLinks(f_url)\n",
    "    #contruct the full path for each link retrieved:\n",
    "    fpu_link = [f_url+local_link for local_link in fpu if local_link.count('/') == 1]\n",
    "    if fpu_link: \n",
    "        fourth_page_urls+=fpu_link\n",
    "        print(fpu_link)\n",
    "        \n",
    "    elif '.xml' in f_url:  #reached the file I need -> download it\n",
    "        file_name = 'musicXMLFiles/' + os.path.basename(f_url)\n",
    "        print(file_name)\n",
    "        try:\n",
    "            urlretrieve(f_url, file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download')   \n",
    "        time.sleep(1)\n",
    "    elif '.' in f_url[-4:-1]:\n",
    "        print(os.path.basename(f_url))\n",
    "        print('no xml file, just other formats') \n",
    "#print(fourth_page_urls)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's go one level deeper and start checking for files:\n",
    "fifth_page_urls =[]\n",
    "for fi_url in fourth_page_urls:\n",
    "    #print(fi_url)\n",
    "    #retrieve the href links from the page\n",
    "    fipu = getLinks(fi_url)\n",
    "    #contruct the full path for each link retrieved:\n",
    "    fipu_link = [fi_url+local_link for local_link in fipu if local_link.count('/') == 1]\n",
    "    if fipu_link: \n",
    "        fifth_page_urls+=fipu_link\n",
    "        print(fipu_link)\n",
    "        \n",
    "    elif '.xml' in fi_url:  #reached the file I need -> download it\n",
    "        file_name = 'musicXMLFiles/' + os.path.basename(fi_url)\n",
    "        print(file_name)\n",
    "        try:\n",
    "            urlretrieve(fi_url, file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download')   \n",
    "        time.sleep(1)\n",
    "    elif '.capx' in fi_url or '.mid' in fi_url:\n",
    "        file_name = 'musicXMLFiles/' + os.path.basename(fi_url)\n",
    "        print(file_name)\n",
    "        try:\n",
    "            urlretrieve(fi_url, file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download')   \n",
    "        time.sleep(1)\n",
    "    else '.' in fi_url[-4:-1]:\n",
    "        print(os.path.basename(fi_url))\n",
    "        print('no xml file, just other formats') \n",
    "#print(fifth_page_urls)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's go one level deeper and start checking for files:\n",
    "sixth_page_urls =[]\n",
    "for s_url in fifth_page_urls:\n",
    "    #print(s_url)\n",
    "    #retrieve the href links from the page\n",
    "    spu = getLinks(s_url)\n",
    "    #contruct the full path for each link retrieved:\n",
    "    spu_link = [s_url+local_link for local_link in spu if local_link.count('/') == 1]\n",
    "    if spu_link: \n",
    "        sixth_page_urls+=spu_link\n",
    "        #print(spu_link)\n",
    "        \n",
    "    elif '.xml' in s_url:  #reached the file I need -> download it\n",
    "        file_name = 'musicXMLFiles/' + os.path.basename(s_url)\n",
    "        print(file_name)\n",
    "        try:\n",
    "            urlretrieve(s_url, file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download')   \n",
    "        time.sleep(1)\n",
    "    elif '.capx' in s_url or '.mid' in s_url:\n",
    "        file_name = 'musicXMLFiles/' + os.path.basename(s_url)\n",
    "        print(file_name)\n",
    "        try:\n",
    "            urlretrieve(s_url, file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download')   \n",
    "        time.sleep(1)\n",
    "    elif '.' in s_url[-4:-1]:\n",
    "        print(os.path.basename(s_url))\n",
    "        print('no xml file, just other formats') \n",
    "#print(fifth_page_urls)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the links to file\n",
    "print(sixth_page_urls)\n",
    "fName = 'hausmusikURLsList1.txt'\n",
    "f1 = open(fName,\"w\") \n",
    "for url_link in sixth_page_urls:\n",
    "    f1.write(url_link)\n",
    "    f1.write(\"\\n\")\n",
    "f1.close()          \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's go through saved urls and start checking for files:\n",
    "fifth_page_urls =[]\n",
    "filename = 'hausmusikURLsList1.txt'\n",
    "\n",
    "with open(filename) as f:\n",
    "    allText = f.readlines()\n",
    "    for line in allText:\n",
    "        fifth_page_urls.append(line[:-1])\n",
    "        \n",
    "        \n",
    "        \n",
    "#print(fifth_page_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileLinks(url_list):\n",
    "    #set a counter to see how often it gets called\n",
    "    counter = 0\n",
    "    #get the content\n",
    "    files = []\n",
    "    for url in url_list:\n",
    "        path = urlparse(url).path\n",
    "        ext = os.path.splitext(path)[1]\n",
    "        print(ext)\n",
    "        if not ext:\n",
    "            #it means we'll get a new list of urls to process, needs to be initialized:\n",
    "            print('no extension yet')\n",
    "            #loop through the linked urls to get all files at the end of this link \n",
    "            new_url_list = getLinks(url)\n",
    "            print(f'nurls {new_url_list}')\n",
    "            \n",
    "            fs = getFileLinks(new_url_list)\n",
    "            if fs:\n",
    "                fName = 'hausmusikURLsList5.txt'\n",
    "                f1 = open(fName,\"w\") \n",
    "                for url_link in fs:\n",
    "                    f1.write(url_link)\n",
    "                    #f1.write(\"\\n\")\n",
    "                f1.close()    \n",
    "                \n",
    "            else:\n",
    "                print('not sure what is going on here')\n",
    "        else:\n",
    "            files += url\n",
    "    counter+=1            \n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "no extension yet\n",
      "nurls ['http://de.hausmusik.ch/', 'http://de.hausmusik.ch/notenregal/', 'http://de.hausmusik.ch/notenregal/a/', 'http://de.hausmusik.ch/notenregal/a/alsen_wulf-dieter/', 'http://de.hausmusik.ch/notenregal/a/alsen_wulf-dieter/oratorium/', 'http://de.hausmusik.ch/notenregal/a/alsen_wulf-dieter/oratorium/weihnachtsgeschichte/', 'http://de.hausmusik.ch/notenregal/a/alsen_wulf-dieter/oratorium/weihnachtsgeschichte/weihn-ganz-01/', 'http://www.hausmusik.ch/notenregal/a/alsen_wulf-dieter/oratorium/weihnachtsgeschichte/weihn-ganz-01/weihn-ganz-01.cap', 'http://www.hausmusik.ch/notenregal/a/alsen_wulf-dieter/oratorium/weihnachtsgeschichte/weihn-ganz-01/weihn-ganz-01.capx', 'http://www.hausmusik.ch/notenregal/a/alsen_wulf-dieter/oratorium/weihnachtsgeschichte/weihn-ganz-01/weihn-ganz-01.mid', 'http://www.hausmusik.ch/notenregal/a/alsen_wulf-dieter/oratorium/weihnachtsgeschichte/weihn-ganz-01/weihn-ganz-01.pdf', 'http://www.hausmusik.ch/notenregal/a/alsen_wulf-dieter/oratorium/weihnachtsgeschichte/weihn-ganz-01/weihnganz01/index.shtml']\n",
      "\n",
      "no extension yet\n",
      "nurls ['', 'http://de.hausmusik.ch/notenregal/', 'http://www.whc.de/', 'http://www.hausmusik.ch/Noten', 'http://de.hausmusik.ch/trommelselbstbau']\n",
      "\n",
      "no extension yet\n"
     ]
    },
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '': No schema supplied. Perhaps you meant http://?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-172aa89fd48e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetFileLinks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifth_page_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hausmusikURLsList5.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl_link\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-4d5519f2bc65>\u001b[0m in \u001b[0;36mgetFileLinks\u001b[0;34m(url_list)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'nurls {new_url_list}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetFileLinks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_url_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mfName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hausmusikURLsList5.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-4d5519f2bc65>\u001b[0m in \u001b[0;36mgetFileLinks\u001b[0;34m(url_list)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'nurls {new_url_list}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetFileLinks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_url_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mfName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hausmusikURLsList5.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-4d5519f2bc65>\u001b[0m in \u001b[0;36mgetFileLinks\u001b[0;34m(url_list)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no extension yet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m#loop through the linked urls to get all files at the end of this link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mnew_url_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetLinks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'nurls {new_url_list}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-cd86c38eecd7>\u001b[0m in \u001b[0;36mgetLinks\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetLinks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#get the content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhtml_page\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#initialize list of urls to files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         )\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_setting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mcookies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_cookies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         )\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingSchema\u001b[0m: Invalid URL '': No schema supplied. Perhaps you meant http://?"
     ]
    }
   ],
   "source": [
    "fs = getFileLinks(fifth_page_urls)\n",
    "if fs:\n",
    "    fName = 'hausmusikURLsList5.txt'\n",
    "    f1 = open(fName,\"w\") \n",
    "    for url_link in fs:\n",
    "        f1.write(url_link)\n",
    "        #f1.write(\"\\n\")\n",
    "    f1.close()    \n",
    "else:\n",
    "    print('not sure what is going on here second round')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_url in fifth_page_urls:\n",
    "    #print(s_url)\n",
    "    #retrieve the href links from the page\n",
    "    spu = getLinks(s_url)\n",
    "    #contruct the full path for each link retrieved:\n",
    "    spu_link = [s_url+local_link for local_link in spu if local_link.count('/') == 1]\n",
    "    if spu_link: \n",
    "    \n",
    "        sixth_page_urls+=spu_link\n",
    "        #save the links to file\n",
    "        print(sixth_page_urls)\n",
    "        fName = 'hausmusikURLsList2.txt'\n",
    "        f1 = open(fName,\"w\") \n",
    "        for url_link in sixth_page_urls:\n",
    "            f1.write(url_link)\n",
    "            f1.write(\"\\n\")\n",
    "        f1.close()    \n",
    "        \n",
    "    elif '.xml' in s_url:  #reached the file I need -> download it\n",
    "        print(os.path.basename(s_url))\n",
    "        if s_url[-1] == '/': \n",
    "            s_url = s_url[:-1]\n",
    "        file_name = 'musicXMLFiles/' + os.path.basename(s_url)\n",
    "        print(file_name)\n",
    "        try:\n",
    "            urlretrieve(s_url, file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download') \n",
    "            print(s_url)\n",
    "        try:\n",
    "            urlretrieve(s_url[:-1], file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download') \n",
    "            print(s_url)\n",
    "        time.sleep(5)\n",
    "    elif '.capx' in s_url or '.mid' in s_url:\n",
    "        if s_url[-1] == '/': \n",
    "            s_url = s_url[:-1]\n",
    "        file_name = 'musicXMLFiles/' + os.path.basename(s_url)\n",
    "        \n",
    "        print(file_name)\n",
    "        try:\n",
    "            urlretrieve(s_url, file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download') \n",
    "            print(s_url)\n",
    "        try:\n",
    "            urlretrieve(s_url[:-1], file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download') \n",
    "            print(s_url)\n",
    "        \n",
    "    elif '.' in s_url[-4:-1]:\n",
    "        print(os.path.basename(s_url))\n",
    "        print('no xml file, just other formats') \n",
    "        \n",
    "#print(sixth_page_urls)\n",
    "seventh_page_urls = []\n",
    "for sv_url in sixth_page_urls:\n",
    "    #print(s_url)\n",
    "    #retrieve the href links from the page\n",
    "    svpu = getLinks(sv_url)\n",
    "    #contruct the full path for each link retrieved:\n",
    "    svpu_link = [sv_url+local_link for local_link in svpu if local_link.count('/') == 1]\n",
    "    if svpu_link: \n",
    "        seventh_page_urls+=svpu_link\n",
    "        #save the links to file\n",
    "        print(seventh_page_urls)\n",
    "        fName = 'hausmusikURLsList3.txt'\n",
    "        f1 = open(fName,\"w\") \n",
    "        for url_link in seventh_page_urls:\n",
    "            f1.write(url_link)\n",
    "            f1.write(\"\\n\")\n",
    "        f1.close()          \n",
    "      \n",
    "\n",
    "        #print(spu_link)\n",
    "        \n",
    "    elif '.xml' in sv_url:  #reached the file I need -> download it\n",
    "        if sv_url[-1] == '/': \n",
    "            sv_url = sv_url[:-1]\n",
    "        file_name = 'musicXMLFiles/' + os.path.basename(sv_url)\n",
    "        \n",
    "        print(file_name)\n",
    "        try:\n",
    "            urlretrieve(sv_url, file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download') \n",
    "            print(sv_url)\n",
    "        try:\n",
    "            urlretrieve(sv_url[:-1], file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download') \n",
    "            print(sv_url)\n",
    "        \n",
    "    elif '.capx' in sv_url or '.mid' in sv_url:\n",
    "        if sv_url[-1] == '/': \n",
    "            sv_url = sv_url[:-1]\n",
    "        file_name = 'musicXMLFiles/' + os.path.basename(sv_url)\n",
    "        \n",
    "        print(sv_url)\n",
    "        try:\n",
    "            urlretrieve(sv_url, file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download') \n",
    "            print(sv_url)\n",
    "        try:\n",
    "            urlretrieve(sv_url[:-1], file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download') \n",
    "            print(sv_url)\n",
    "    elif '.' in sv_url[-4:-1]:\n",
    "        print(os.path.basename(sv_url))\n",
    "        print('no xml file, just other formats') \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sixth_page_urls)\n",
    "seventh_page_urls = []\n",
    "for sv_url in sixth_page_urls:\n",
    "    #print(s_url)\n",
    "    #retrieve the href links from the page\n",
    "    svpu = getLinks(sv_url)\n",
    "    #contruct the full path for each link retrieved:\n",
    "    svpu_link = [sv_url+local_link for local_link in svpu if local_link.count('/') == 1]\n",
    "    if svpu_link: \n",
    "        seventh_page_urls+=svpu_link\n",
    "        #print(spu_link)\n",
    "        \n",
    "    elif '.xml' in sv_url:  #reached the file I need -> download it\n",
    "        file_name = 'musicXMLFiles/' + os.path.basename(sv_url)\n",
    "        \n",
    "        print(file_name)\n",
    "        try:\n",
    "            urlretrieve(sv_url, file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "        except:\n",
    "            print('failed to download') \n",
    "            print(sv_url)\n",
    "        time.sleep(1)\n",
    "    elif '.capx' in sv_url or '.mid' in sv_url:\n",
    "        file_name = 'musicXMLFiles/' + os.path.basename(sv_url)\n",
    "        \n",
    "        print(file_name)\n",
    "        try:\n",
    "            urlretrieve(sv_url, file_name)\n",
    "            print(f'downloaded file {file_name}')\n",
    "            \n",
    "        except:\n",
    "            print('failed to download') \n",
    "            print(sv_url)\n",
    "        time.sleep(5)\n",
    "    elif '.' in sv_url[-4:-1]:\n",
    "        print(os.path.basename(sv_url))\n",
    "        print('no xml file, just other formats') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_url = 'https://memory.loc.gov/diglib/ihas/loc.music.sm1854.730990/mex.xml'\n",
    "#try:\n",
    "file_name = 'musicXMLFiles/'+os.path.basename(fi_url)\n",
    "print(os.path.basename(fi_url))\n",
    "print(os.path.basename(fi_url)[:-1])\n",
    "#urlretrieve(fi_url, file_name)\n",
    "print(f'downloaded file {file_name}')\n",
    "#except:\n",
    "    #print('failed to download') \n",
    "path = urlparse(fi_url).path\n",
    "ext = os.path.splitext(path)[1]\n",
    "if ext: \n",
    "    print(' a file')\n",
    "else:\n",
    "    print(' not a file')\n",
    "    \n",
    "testPath ='https://memory.loc.gov/diglib/ihas/loc.music.sm1870.x0016/mets.xml'\n",
    "urlretrieve(testPath,file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
